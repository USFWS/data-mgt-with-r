---
title: "R tools for a code-based data workflow"
author: "McCrea Cobb and Adam D. Smith"
date: '`r Sys.Date()`'
output:
  xaringan::moon_reader:
    lib_dir: libs
    css: ["css/xaringan-themer.css", "css/my-theme.css"]
    nature:
      ratio: "16:9"
      beforeInit: "macros/macros.js"
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      navigation:
        scroll: false
    seal: false
    includes:
      after_body: usfws-logo.html
editor_options: 
  chunk_output_type: console
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE, 
                      fig.retina = 3, fig.align = "center")

options(htmltools.dir.version = FALSE)
library(icon)
library(tidyverse)
library(crayon)
library(flair)
library(sbtools)
```

```{r xaringan-themer, include=FALSE, eval=FALSE}
library(xaringanthemer)
mono_light(
  code_font_family = "Fira Code",
  code_font_url    = "https://cdn.rawgit.com/tonsky/FiraCode/1.204/distr/fira_code.css"
)
```


class: hide-logo, center, middle, inverse
# ![:scale 9%](images/Rlogo.svg) Tools for a Code-based Data Workflow

.pull-left[
`McCrea Cobb`  
`r icon::fa("envelope")` mccrea_cobb@fws.gov  
`r icon::fa("phone")` 907-786-3403  
`r icon::fa("github")` mccrea.cobb  
]

.pull-right[
`Adam D. Smith`  
`r icon::fa("envelope")` adam_d_smith@fws.gov  
`r icon::fa("phone")` 706-425-2197  
`r icon::fa("github")` adamdsmith  
]

<br>

`r icon::fa("github")` usfws.github.io/data-mgt-with-r/


???
Hello and welcome to Tools for a Code-based Data Workflow. 

My name is McCrea Cobb. I work as a biometrician in the Inventory and Monitoring Program within the National Wildlife Refuge Sytem, and I am located in Anchorage, Alaska. I am co-presenting with Adam Smith, who is quantitative ecologist with the I&M program based in Athens, Georgia.

You can find our presentation slides at the link at the bottom.


---
class: inverse, center, middle, hide-logo
# Outline


???
I'll start out by providing a brief outline of what we will be covering today.


---
class: inverse, center, middle, hide-logo
# Outline

.left[.pull-right-70[
#### Review the data life cycle and data workflow
]
]


???
First, we will review some of the concepts introduced earlier today by Jared and Maren. We will provide a cursory review of the data and project life cycles and how these models fit into scientific data workflows. We will compare what we're calling the "traditional" workflow that typically involves manually manipulating data with "point and clicks" using propritary software to a code-based data workflow in R.


---
class: inverse, center, middle, hide-logo
# Outline

.left[.pull-right-70[

<span style="color:gray">Review the data life cycle and data workflow</span>

#### Present tools in ![:scale 3%](images/Rlogo.svg) for efficiently and effectively working with data along the life cycle
]
]


???
Next, we will use steps of a data life cycle, from Planning to Archiving, as a guide to  present tools and tips for developing a data workflow in R, a programming language for statistical computing and graphics.


---
class: inverse, center, middle, hide-logo
# Outline

.left[.pull-right-70[
<span style="color:gray">Review the data life cycle and data workflow</span>

<span style="color:gray">Present some tools in ![:scale 3%](images/Rlogo.svg) for efficiently and effectively working with data along the life cycle</span>

#### Demonstrate a data workflow in ![:scale 3%](images/Rlogo.svg) using an example project
]
]


???
Finally, we will demonstrate of how these tools and concepts can be applied to real-world data, using a FWS survey case study focused on mobile acoustical bat monitoring from southeastern US.

Our focus is limited to scientific data, but the tools and tips that we cover apply to a wide range of data.


---
background-image: url(images/draw_owl-kosher.jpg)
background-size: contain
class: hide-logo

???
Before starting though, we wanted to include a disclaimer. It can be frustrating trying to learn a new language like R. Much like learning to draw well, you cannot learn R in a hour and a half. If we tried to do this, it might come around like this image. 

-So, with our limited time, we decided to focus on the application of R to a data work. This webinar is not an "Introduction to R" or a "How to get your data into R" or "Learn the Tidyverse".

- Our take home message is simply: It is possible to use a scripting languaging like R to complete a scientific data workflow that largely mirrors the data life cycle. Taking this approach has many advantages: including, it is efficient, reproducible, and less error prone than a point and click data workflow.

  - If you are interested in learning R, there are many courses that cover these materials, from online courses from DataCamp or in-person courses through NCTC courses (CSP1004 Data Wrangling in R)
  
  - We have included links in the slides to additional resources and we encourage you to visit our github site where there are many more additional resources listed.


---
# Project and data life cycles

.center[![:scale 80%](images/life_cycles.png)]


???
- As I mentioned, we will follow the project and data life cycles that Jared and Maren introduced in the previous webinar on data management as our guide for introducing R tools. 
- As you can see here, both the data and project life cycles start with a **Planning** step, and so we will start with recommended actions that we think should be considered during the planning step and cover some associated tools in R.


---
background-image: url(images/life_cycles-data.png)
background-size: 90%
class: hide-logo


???
- After that, we will progress through th rest of the data life cycle: starting with the Acquire step, and moving through the Process, Analyze, Share and Achive steps. We will also cover documenting in R.


---
class: hide-logo
# `r icon::fa("times", color = "red")` Traditional data workflow


???
Ok. So how are these steps typically accomplished? Let's first walk through the traditional point and click data workflow that is probably familiar to everybody here.


---
background-image: url(images/traditional_workflow_1.png)
background-size: 90%
class: hide-logo
# `r icon::fa("times", color = "red")` Traditional data workflow


???
First, you start with data. These could be paper datasheets, maybe some data that were collected using a mobile app like Collector or Survey123, or data that you found in a online repository like ServCat or data.gov. 


---
background-image: url(images/traditional_workflow_2.png)
background-size: 90%
class: hide-logo
# `r icon::fa("times", color = "red")` Traditional data workflow


???
The first step is to acquire these data. This is generally done by digitizing paper data into Excel,  Access or ArcGIS, and downloading data from a repository by navigating to a webpage in Chrome and clicking on button. 


---
background-image: url(images/traditional_workflow_3.png)
background-size: 90%
class: hide-logo
# `r icon::fa("times", color = "red")` Traditional data workflow


???
Next, these data are processed and quality-checked using these same software. Occasionally, people might load data into R, process the data in R and then export the clean dataset back as a csv, giving it a new name like "data_clean" for version control and to differentiate it from the raw data.


---
background-image: url(images/traditional_workflow_4.png)
background-size: 90%
class: hide-logo
# `r icon::fa("times", color = "red")` Traditional data workflow


???
The third step is data analysis. For spatial data, this step is often done in ArcGIS. For exploratory data analysis of non-spatial data, this might be a accomplished using Excel or Access. More complex analyses might require the user to convert the data to a csv, load it into R, run from models, and write CSV table of results. 


---
background-image: url(images/traditional_workflow_5.png)
background-size: 90%
class: hide-logo
# `r icon::fa("times", color = "red")` Traditional data workflow


???
Reporting these results requires the user to write summary text in a Word document or in Powerpoint slides. ArcGIS, Excel, and R are generally used to create figures that are often saved as JPGs. 


---
background-image: url(images/traditional_workflow_6.png)
background-size: 90%
class: hide-logo
# `r icon::fa("times", color = "red")` Traditional data workflow `r icon::fa("frown-open")`


???
Finally, to complete the report or presentation slides, the user is required to manually copy and paste the JPG figures into Word or Powerpoint.


---
class: center, middle, hide-logo

![](images/criteria_short.png)


???
How do we evaluate whether this is a "good" data workflow?

To do this, we need criteria from which to measure success. You can probably imagine a long list of these, but for this webinar, we'll focus on these six.

A good data workflow should maximize the criteria listed here. 

Let's evaluate the traditional data workflow using these criteria. 
 
- First, is it is reproducible? Not really. If someone else were attempting to reproduce this effort, they might not know which data were used to generate the figures and tables and they are even less likely to be able to replicate all the steps in the workflow. 

- Is it replicable by the person that completed the workflow? Unlikely. The person that completed the workflow might have trouble replicating the steps for the next year's annual report.

- Is it efficient? Perhaps for a single one-off project, but not for any long-term data collection effort. Almost all these steps have to be repeated to reproduce the results with new data.

- Is it standardized? No, even small details like the sizes of figures is likely differ between analyses.

- Is it scalable. No. Every time that new data are generated or the original data change, then the workflow breaks and needs to be restarted, sometimes from scratch. 
    
    
---
class: center, middle, hide-logo

![](images/criteria_short_xed.png)


???
boo..


---
background-image: url(images/workflow.gif)
background-size: contain
class: hide-logo


???
- What we need is a seamless workflow like this, where the data go from acquiring (i.e. hand) to process (and archiving).
- Your code should be the source of your products (report, etc). 


---
class: hide-logo
# `r icon::fa("check", color = "green")` ![:scale 7%](images/Rlogo.svg) data workflow 


???
So how does this compare to a code-based workflow?


---
background-image: url(images/r_workflow_1.png)
background-size: 90%
class: hide-logo
# `r icon::fa("check", color = "green")` ![:scale 7%](images/Rlogo.svg) data workflow 


???
The input data are often the same, although eliminating paper datasheets can eliminates transcription error and, as Adam will touch on later, allows us to acquire data in R directly through ArcGIS Online's API.


---
background-image: url(images/r_workflow_3.png)
background-size: 90%
class: hide-logo
# `r icon::fa("check", color = "green")` ![:scale 7%](images/Rlogo.svg) data workflow


???
As we will show, R can directly import data from a variety of sources. This is now easier than ever thanks to a number of R package that include flexible data import functions, including spatial data.


---
background-image: url(images/r_workflow_5.png) 
background-size: 90%
class: hide-logo
# `r icon::fa("check", color = "green")` ![:scale 7%](images/Rlogo.svg) data workflow


???
R offers a number of tools for quality control, including both passive data profiling and active assertive programming approaches. More to come on that. Where R starts to really shin tidying up mess data and there are a whole suite of packages called the "tidyverse" that address this step.


---
background-image: url(images/r_workflow_6.png)
background-size: 90%
class: hide-logo
# `r icon::fa("check", color = "green")` ![:scale 7%](images/Rlogo.svg) data workflow


???
R was originally developed as a statistical software and there are now too many packages to even start listing that allow for a ever growing number of analytical approaches.


---
background-image: url(images/r_workflow_8.png)
background-size: 90%
class: hide-logo
# `r icon::fa("check", color = "green")` ![:scale 7%](images/Rlogo.svg) data workflow


???
Maybe less known is the ability to generate reports using a mixture of text and R code. Recently, a number of packages have been developed to address this need that we will touch on.


---
background-image: url(images/r_workflow_9.png)
background-size: 90%
class: hide-logo
# `r icon::fa("check", color = "green")` ![:scale 7%](images/Rlogo.svg) data workflow


???
These code-based reports can be output in number of file format, including html, PDF and Word documents. Even more importantly, it is possible to archive these data and data products back to repositories from R, thereby closing the data life cycle loop. 


---
background-image: url(images/r_workflow_10.png)
background-size: 90%
class: hide-logo
# `r icon::fa("check", color = "green")` ![:scale 7%](images/Rlogo.svg) data workflow `r icon::fa("grin-stars")`


???
Last but not least, by coding these steps all within an R environment, it is possible to wrap up these steps into a reusable R package. 


---
background-image: url(images/Rlogo.svg)
class: hide-logo, middle, right
background-size: 40%
background-position: 10% 50%

![:scale 50%](images/rstudio_logo.png)


???
This presentation focuses on R and RStudio, which is a integrated development environment (IDE) for R to develop and share work.

- Why R?
  - Free for everybody
  - Relatively easy to learn (compared to other programming languages)
  - Powerful
  - Flexible
    - statistical analyses
    - graphics
    - reporting
  - Nice free integrated development environment (RStudio)
  - Popular (2 millions users)
      - Active community of users
      - package development - currently over 15,000 packages
  - GitHub integration


---
class: center, middle, inverse

# Planning
![](images/life_cycles-data_planning.png)


???
Ok. Let's get with the first step: Planning 


---
# Organizing an ![:scale 7%](images/Rlogo.svg) project 

### `r icon::fa("sitemap")` Chose a standardized working directory structure

.pull-left[
Provides consistent relative directory paths for your scripts

- R packages provide functions to create a standard file directory:
    - [`MakeProject::MakeProject()`](https://cran.r-project.org/web/packages/makeProject/index.html)
    - [`rrtools::use_analysis()`](https://www.rdocumentation.org/packages/rrtools/versions/0.1.0)
    - [`refugetools::create.dir()`](https://github.com/USFWS/refugetools)
    - [`prodigenr::setup_project()`](https://cran.r-project.org/web/packages/prodigenr/readme/README.html)

  
]

.pull-right[
An example directory structure:
```{r, eval=FALSE}
project_name/
  admin/
  code/ #<<
    functions/
  data/ #<<
    derived_data/
    raw_data/
  incoming/
  metadata/
  output/
    figures/
    raw_analysis/
    tables/
  products/
  resources/ #<<
    data/ 
    publications/
    reports/
```
]


???
There are many decision that need to be made when starting project involving data. Perhaps one of the first is how to organize your work. While not necessarily a data management issue, having a standardized project directory has advantages when using a code-based workflow.

- Project-oriented workflows are self-contained workflows enabling reproducibility and replicability. Ideally a collaborator or the next biologists should be able to run the entire project without changing any code or files. File paths should be workstation-independent. 

- R has options for enabling self-contained workflows in their coding environments, and RStudio projects allow for analyses to be contained in a single working directory that can be given to a collaborator and run without changing file directory paths. 

- The figure on the right shows an example of a working directory file structure. Notice that all data produced by the project are stored in a folder called data. Centralizing data like this makes the file paths in R consistent, and it also makes it easier to 

- There are tools in R that produce a standardized directory structure. We don't have time today to go into detail about each of these, but there are links here to learn more about them.
    - Examples: 
        - MakeProject package
        - SppDistMonProj:: dir_create()


---
# Organizing an ![:scale 7%](images/Rlogo.svg) project 

### `r icon::fa("file-signature")` Decide on a standardized file naming convention

.pull-left[
- Call files what they are
- Keep names short
- Avoid spaces and $pec!@l characters
- If dates are used, use YYYYMMDD
- Decide on a standard

Allows you to programmatically filter files:
```{r, eval=FALSE}
# Exclude files with 2016 or 2017 in their names
f <- list.files(path = "./filepath", 
                pattern = "[^2016|2017].csv$") #<<

# Load these files
dat <- lapply(f, read.csv)
```
]

.pull-right[
![Allison Horst](https://raw.githubusercontent.com/allisonhorst/stats-illustrations/master/other-stats-artwork/coding_cases.png)
]


???
Data managers might tell you that it doesn't matter what you name your files, as long as they are full documented and archived. Although descriptive file names might be required for data management, there are advantages to having a standardized file naming convention for R project management. Descriptive file names are an important part of organizing, sharing, and keeping track of data files.

Some general guidance includes: call files what they are, keep file names short, avoid spaces and special characters (which might not play well with some software), use a standard for dates such as ISO 8601. 

Deciding on a standard and using it for everything is a good approach. The figure on the right shows a few common naming standards, such as camel case and snake case.

Sticking to a standard allows you to do things in R that would be difficult otherwise, such as programmically filter files. For example, the code at the bottom imports a list of data files but excluded those containing 2016 or 2017.  


---
# Organizing an ![:scale 7%](images/Rlogo.svg) project 

### `r icon::fa("book")` Consult a style guide

*"Good coding style is like correct punctuation: you can manage without it, butitsuremakesthingseasiertoread"*  
*-[Hadley Wickham](https://style.tidyverse.org/index.html)*

.pull-left[
- Strive for consistent and meaningful names

- Review existing style guides:
  - [tidyverse style guide](https://style.tidyverse.org/)
  - [Advanced R style guide](http://adv-r.had.co.nz/Style.html)
  - [Google's R style guide](https://google.github.io/styleguide/Rguide.html)

- Helpful R packages:
  - [`styler`](https://www.tidyverse.org/blog/2017/12/styler-1.0.0/) 
  - [`lintr`](https://cran.r-project.org/web/packages/lintr/index.html)
]

.pull-right[
```{r, eval=FALSE, results='markup'}
# R code readability

if(readability()) {
  be_happy()
} else {
  rewrite_code()
}
```
]

???

In addition deciding on a standardized file naming convention during the planning step, we should think about settling on a consistent coding syntax, such as object names, spacing, long lines, assignments, comments. Again, this is not a requirement for good data management and you can manage without it, but it sure makes things easier to read. Do it for your future self and anyone else that has to read through your code.

A good starting place is to consult a style guide. There are a number of them out there, but I list a few of the more popular ones for R. 

If you are past the planning stage and are struggling to make sense of ugly code, there are an R package called `styler` that has functions to read through and clean up code to a standard of your chosing. Another package called `lintr` will perform on the fly checking to confirm that you are conforming to the style guide. Links to more information are included here.

I'll talk about in more detail on the next slide. 
 
 
---
# [`styler`](https://github.com/r-lib/styler) package

.pull-left[
Some messy code `r icon::fa("flushed", color = "grey")`

.small[
```{r messy1, include=FALSE, eval=FALSE, message=FALSE, warning=FALSE}
p =dat%>%#a comment
select(refuge ,lat, long) %>%filter(refuge!='kodiak ')%>%
  ggplot(data,aes(x =lat,y= long,group=refuge))%>%  geom_line()#comment without space"
```

```{r, echo=FALSE}
decorate("messy2", eval=FALSE) %>%
  flair("styler") %>%
  flair("select", background = "pink") %>%
  flair("filter", background = "pink") %>%
  flair("ggplot", background = "pink") %>%
  flair("geom_line", background = "pink") %>%
  flair("a comment", color="blue") %>%
  flair("comment without space", color = "blue")
```
]
]


???


---
# [`styler`](https://github.com/r-lib/styler) package

.pull-left[
Some messy code `r icon::fa("flushed", color = "grey")`

.small[
```{r messy2, include=FALSE, eval=FALSE, message=FALSE, warning=FALSE}
p =dat%>%#a comment
select(refuge ,lat, long) %>%filter(refuge!='kodiak ')%>%
  ggplot(data,aes(x =lat,y= long,group=refuge))%>%  geom_line()#comment without space"
```

```{r, echo=FALSE}
decorate("messy2", eval=FALSE) %>%
  flair("styler") %>%
  flair("select", background = "pink") %>%
  flair("filter", background = "pink") %>%
  flair("ggplot", background = "pink") %>%
  flair("geom_line", background = "pink") %>%
  flair("a comment", color="blue") %>%
  flair("comment without space", color = "blue")
```
]
]

.pull-right[
The `style_text()` function cleans it up! `r icon::fa("smile-beam", color = "grey")`
.small[
```{r cleaned, include=FALSE, message=FALSE, warning=FALSE, comment=""}
library(styler)

style_text("p =dat%>%#a comment
select(refuge ,lat, long) %>%filter(refuge!='kodiak ')%>%
  ggplot(data,aes(x =lat,y= long,group=refuge))%>%  geom_line()#comment without space") 
```

```{r, echo=FALSE}
decorate("cleaned", eval=TRUE) %>%
  flair("styler") %>%
  flair("select", background = "pink") %>%
  flair("filter", background = "pink") %>%
  flair("ggplot", background = "pink") %>%
  flair("geom_line", background = "pink") %>%
  flair("a comment", color="blue") %>%
  flair("comment without space", color = "blue")
```
]
]

???


---
# `r icon::fa("people-carry")` Project portability


.pull-left-30[
Maintaining dependencies can be frustrating!

An R project should be: 

1. **Isolated**
2. **Portable**
3. **Reproducible**

There are R tools to help with this:
- [`packrat`](https://rstudio.github.io/packrat/) package
- `rocker` package
]

.pull-right-70[
![:scale 100%](images/package-hell.png)
]


???
Ideally, a data project should be isolated, portable and reproducible.

To be isolated mean that: 
  - Installing a new or updated package for one project won’t break your other projects, and vice versa. That’s because packrat gives each project its own private package library.

To be portable means that: 
  - it should be easy to transport your projects from one computer to another, even across different platforms. Packrat makes it easy to install the packages your project depends on.

To be reproducible means that:
  - It is clear to everyone which software versions are required to successfully rerun code. When new versions come out, they might not be backward compatible. Certain functions might no longer be available. Packrat records the exact package versions you depend on, and ensures those exact versions are the ones that get installed wherever you go.

- Available tools:
  - packrat
  - rocker
    - Docker is a program that allows to manipulate (launch and stop) multiple operating systems (called containers) on your machine (your machine will be called the host).
    - you can use older versions of a package for a specific task, while still keeping the package on your machine up-to-date.


---
# ![:scale 8%](images/packrat_logo.png)

.pull-left[
Option 1: Add to a new project

![](images/packrat_rstudio.PNG)
]

.pull-right[
Option 2: Add to an existing project

.small[
```{r packrat1, include=FALSE, eval=FALSE}
# Install packrat
install.packages('packrat')

# Set up your project to use packrat
packrat::init()
```

```{r, echo=FALSE}
decorate("packrat1", eval=FALSE) %>%
  flair("init()") %>%
  flair("install.packages('packrat')", background = "pink")
```
]

.small[
```{r packrat2, include=FALSE, eval=FALSE}
# Install required packages
install.packages('tidyverse')

# Take a snapshot to save the changes to packrat
packrat::snapshot()
```

```{r, echo=FALSE}
decorate("packrat2", eval=FALSE) %>%
  flair("snapshot()")
```
]
]

???


---
background-image: url("images/version_control.png")
background-size: 55%
background-position: 95% 70%
# `r icon::fa("code-branch")` Version control 

.pull-left-30[
<br>
<br>
.center[
.large[Consider how you will manage versions when ***planning*** your project!]
]
]

???
Version control is the processes and tools designed to keep track of multiple different versions of data, including data products. It can be as simple as renaming the updated files to as complex as a distributed version control system like Git. Whatever the level of complexity, it is important to consider what version control system that a project will be using during the *planning stage*.  

---
# `r icon::fa("code-branch")` Version control 

.pull-left[
- Benefits of a version control system
    - Collaboration
    - Storing versions
    - Restoring versions
    - Understanding what happened
    - Backup

- RStudio makes version control with Git and GitHub easier. 

- References
  - Resources to learn git [(link)](https://try.github.io/)
  - Using git from RStudio [(link)](https://nceas.github.io/oss-lessons/version-control/4-getting-started-with-git-in-RStudio.html)
  - GitHub and R: An Intro for FWS Biologists [(link)](https://mccrea-cobb.github.io/r-github-presentation/#1)
]

.pull-right[
![:scale 60%](images/git_logo.png)
.center[![:scale 40%](images/github.svg)]
.center[![:scale 40%](images/github-logo.png)]
]


???


---
class: center, middle, hide-logo

<video controls autoplay width="1200">
  <source src="images/clone_repo.mp4" type="video/mp4">
</video>

???
- Version control is.. 
- You should consider how you will be managing the version history of your work during the planning stage.
- Git is a popular version control software and GitHub is a companion website.
  - Allows for collaboration as well.
- GitHub is popular with R users because it interfaces with package development


---
class: inverse, center, middle, hide-logo

# Document
![](images/life_cycles-data_documenting.png)

???

Before we dig into data acquisition, processing, and analysis, we need to have 
"THE TALK". Yep, we need to talk about documentation. 

---

# Why document our code?

.pull-left[
.large[**Reproducibility**]

Make it reproducible for:
- colleagues
- future **you**!
]

.pull-right[
.left[[![:scale 50%](images/xkcd-future_self.png)](https://m.xkcd.com/1421/)]
]

???

Why should we document our code? There are plenty of good reasons to document our code, but we'll focus on one --- reproducibility. It's one of criteria McCrea mentioned that we should strive for in the data management life cycle. To put it simply, if you and I start with the same set of data, and a road map for data manipulation and analysis, we should at the very least be able to arrive at the same analytical or graphical results. Arguments about whether our particular analytical road map was "correct" are independent of reproducibility. Reproducibility makes sure we can start those arguments from the same place. McCrea and I are asserting that reproducibility it much easier in a code-based workflow. What does that have to do with documentation? Well, code is written for computers. Documentation helps turn the "computer-speak" of code into human readable content so we can better understand our mutual starting point for discussion/argument. Those humans will often be your colleagues, but are more than likely to be you, some time into the future. And good documentation discourages the self-loathing that often accompanies the experience or revisiting your code 6 months after you started it...

---

# Documenting basics

- [Wilson et al. 2014](https://journals.plos.org/plosbiology/article?id=10.1371/journal.pbio.1001745) *Best practices for scientific computing*

- [Wison et al. 2017](https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1005510) *Good enough practices in scientific computing*

- [Lee 2018](https://doi.org/10.1371/journal.pcbi.1006561) *Ten simple rules for documenting scientific software*

???

Most of the documentation "basics" we cover are described well in these three sources, along with more rigorous documentation techniques. So we simply leave these references here for posterity. But they are worthy reads once you start down the slippery slope of a code-based workflow. 
---
class: despaced

# Documenting basics

.pull-left[
<br>
<br>
.left[![:scale 100%](images/document-it-mkay.jpg)]]

.pull-right[
- Comment code as you go

- Name objects informatively

- Follow a style guide

- Make dependencies explicit

- Write modular code

- Record working environment

- Include a project README

- Version control
]


???

Let me hit you with all of these "basics" of documentation at once, as we'll take each in turn. But it's worth pointing out there's nothing R-specific about this list. They apply equally to your forays into Python, C++, Matlab, or any other coding program. But since this *is* a presentation about R, we'll see a brief example or two of how these basics "look" in R as we work our way through them.

---

# Comment code as you write it

.pull-left[
- "Lab notebook" for code

- Write for people, not computers

- Bare minimum documentation
]
.pull-right[.right[![:scale 100%](images/disturbing.jpg)]]

???

Our first stop on the documentation train is adding comments to the code you write. COmments give you the opportunity to integrate a "lab notebook" into your code. Code is written for computer understanding, comments are written for human understanding. Consider commenting your code as the bare minimum requirement of documenting your code. 

---

# Commenting R code

- Comment marker in ![:scale 3%](images/Rlogo.svg): `#` (number sign/hash)

```{r echo = TRUE, eval = FALSE}
# This is a comment

x <- 2 # This is also a comment
```

--

- <ins>Barest minimum</ins>: brief explanatory description at top of file

.large[
```{r echo = TRUE, eval = FALSE}
# This code retrieves historical METAR weather from NOAA 
# buoys; see http://www.ndbc.noaa.gov/. This function 
# wraps rnoaa::buoy() to allow multiple year queries
```
]

---

# Comment code as you write it

- <ins>Better</ins>: comment **succintly** throughout code

- Focus on your intentions, not mechanics: **"why"** not "what" nor "how"

.center[[![:scale 70%](images/abstrusegoose-bridge.png)](https://abstrusegoose.com/432)]

???

But if you want to be the star student, or at least not hate yourself later, a better approach is to provide thoughtful comments throughout the code that informs the reader, *succintly*, of your intentions. This isn't play-by-play. We don't need to rehash "what" we're doing with the code ("loading in my data"), or "how" the code is doing it ("looping over individuals"), but rather "why" we wrote any particular piece of code. What is that code intending to accomplish?

---
class: despaced

# Comment code as you write it

<br>
**LESS HELPFUL**

```{r echo = TRUE, eval = FALSE}
# Import data
df <- rio::import("survey_data.xlsx")

# Define new variable
df <- mutate(df, above_water = above / (above + below))
```

--

**BETTER**

```{r echo = TRUE, eval = FALSE}
# Import manatee aerial survey data
df <- rio::import("survey_data.xlsx")

# Calculate proportion of manatees above water surface at each location
df <- mutate(df, above_water = above / (above + below))
```

---

# Name objects informatively

.pull-left[

**LESS HELPFUL**

```{r echo = TRUE, eval = FALSE}
# Data object naming
df <- rio::import("birds.csv")
df2 <- rio::import("veg.csv")

# Variable naming
r <- 100
a <- pi * r ^ 2

# Function naming
my_fun <- function(r) {pi * r ^ 2}
```

]

.pull-right[

**BETTER**

```{r echo = TRUE, eval = FALSE}
# Data object naming
bird_counts <- rio::import("birds.csv")
veg_data <- rio::import("veg.csv")

# Variable naming
survey_radius <- 100
survey_area <- pi * survey_radius ^ 2

# Function naming
calc_circle_area <- function(radius) {pi * radius ^ 2}
```

]

???

Comments are one very useful way to help translate the code written for computers into human-readable content. Another important way to integrate human-readable language into our code, without really impacting our software's ability to interpret that code, is to give informative names to the objects we create.

On the left...

---

# Follow (or at least read) a style guide

.center[[![:gen 75%, shadow](images/xkcd-code_quality.png)](https://xkcd.com/1513/)]

???

So, thoughtful commenting and informative object naming are the two main ways you can alter the content of your code to better express your intentions and goals. 

But there are several other documentation "basics" that help you to better organize your code to make it more readable and easier to troubleshoot. The first of these is straightforward---when you write code, do it with a consistent style. McCrea covered this earlier in the presentation, so consider this a friendly reminder to at least read a style guide. 

We link several in the repository associated with this presentation. Whatever style you choose, being consistent with it can dramatically improve the readability of your code, making your intentions that much easier to follow.

---
# Make dependencies explicit

### Package dependencies of code

- Load required packages at beginning of script

```{r load-packages, echo=FALSE, tidy=FALSE}
decorate('
# General usage
library(package)

# Example
library(dplyr)
', eval = FALSE) %>%
  flair_rx("package|dplyr", background = "#CBB5FF") %>%
  knit_print.with_flair()  
```

---
# Make dependencies explicit

### Function dependencies (package source)

- Identify the package source of used functions
- Avoids conflicts when multiple packages share function names 

```{r explicit-dependency, echo=FALSE, tidy=FALSE}
decorate('
# General usage
package::function()

# Examples
rio::import()
dplyr::mutate()
', eval = FALSE) %>%
  flair_rx("package|rio|dplyr", background = "#CBB5FF") %>%
  flair_rx("function|import|mutate", background = "#FFDFD1") %>% 
  knit_print.with_flair()
```

---

# Modularize your code

- Separate scripts for separate tasks

- Execute or load the functionality with the `source` function

```{r echo = TRUE, eval = FALSE}
source("code/00_load_dependencies.R")
source("code/01_import_data.R")
source("code/02_tidy_data.R")
source("code/03_fit_regression_models.R")
source("code/04_generate_figures.R")
source("code/05_generate_report.R")
```

???

It's much easier to navigate through code that has been divided into multiple scripts that perform a focused, specific purpose. This is compared to, say, generating a single script that sequentially proceeds through much of the data management lifecycle. Even the best documentation in the world isn't going to make such a script digestible or easy to navigate.

In R, we can execute the code, or load the functionality of the code, in these separate, modular scripts with the `source` function.

---

# Turn repeated code tasks into functions

### Function structure in ![:scale 3%](images/Rlogo.svg)

```{r functions, include = FALSE, echo = FALSE}
# General usage
functionName <- function(argument1, argument2, ...) {
  # Do stuff
}

# Example
createAwesomePlot <- function(data, 
                              outpath = "plot.png") {
  # Make awesome plot
}
```

.smaller[
```{r, echo = FALSE}
decorate("functions") %>%
  flair_rx("functionName|createAwesomePlot", background = "#CBB5FF") %>%
  flair_rx("argument1|argument2|\\.\\.\\.|data|outpath", background = "#FFDFD1")
```
]

???

Regardless of whether we modularize our code, repeating code (or very nearly repeating code) is a common occurrence. Often we want to perform similar actions, make similar data transformations, generate similar plots, or repeat analyses. Well, in the spirit of avoiding long scripts, and breaking your code up into more manageable, modular chunks, consider turning those sections of repetitive code into functions.

In R, the general structure of functions look like...

---
count: false

```{r, eval = FALSE}
source("code/functions/createAwesomePlot.R")
figure_2 <- createAwesomePlot(data = bird_counts, outpath = "output/birds_plot.png")
figure_3 <- createAwesomePlot(data = veg_data, output = "output/veg_plot.png")
```

.center[![:scale 60%](images/function.jpg)]

???

It's extremely satisfying to write a stand-alone function, `source`ing that functionality into R, and then performing some task with a line of code. It's easier to troubleshoot too...

---

# Documentation for (nearly) free!

- Automated documentation tools
- [`roxygen2`](https://cran.r-project.org/web/packages/roxygen2/vignettes/roxygen2.html)
  - specialized comments (`#'`) translated into documentation

.center[![:scale 45%](images/false.jpg)]

???

While we're on the topic of turning repetitive code into functions, let's bring up another perk beside easier maintenance and the satisfaction of calling your own functions - automated documentation tools

Since you'd be commenting your functions anyway (right?), R has a handy package (roxygen2) that allows you turn basically turn your comments into standardized, human readable, html documentation. You accomplish this through the use of a special comment character (roxygen comment) and a few special tags (e.g., to identify the parameters, product, and examples, etc. of your function)

---

# `roxygen2` example

.center[![:scale 80%](images/get_buoy_roxygen.png)]

---
count: false

# `roxygen2` example

.center[![:scale 80%](images/get_buoy_man.png)]

---

# Record working environment

- Version info for ![:scale 3%](images/Rlogo.svg), the OS, and packages

- `sessionInfo()` or `devtools::session_info()`

???

McCrea mention a couple of ways to wrap all your dependencies up into a single
package prior to sharing to help avoid issues with dealing with different 
operating systems and R package versions

---

# Include a README file

- Briefly introduce your project to users before they access the code

.center[![:scale 60%](images/morpheus.jpg)]

---
# Include a README file

### General Structure

1. What your code is/does (with context)

2. Demonstrate how to use your code (install, configure, find help)

3. What your code looks like in action (examples)

4. Other relevant details (license, acknowledgments, working environment)

---
# Include a README file

### Handy templates available

- e.g., [`usethis::use_readme_rmd`](https://usethis.r-lib.org/reference/use_readme_rmd.html)

### Bonus points: create a [vignette](http://r-pkgs.had.co.nz/vignettes.html)

---

# Put documentation under version control

- Stays "synced" with code as it changes

.center[![:scale 60%](images/dangerous.jpg)]


???

The same benefits of putting your code under version control (with Git/Github for example)
apply to the documentation. When code and the documentation change substantially, you
retain a record of all past versions and can better manage keeping your documentation in step with your code.

---
class: inverse, center, middle, hide-logo

# Acquire
![](images/life_cycles-data_acquire.png)


???

On to data acquisition. In our current context of a code-based workflow, we're not referring to the actual collection of data, say, as a part of field work in a biological monitoring program.(REFERENCE EARLIER WEBINAR RE DIGITATL DATA COLLECTION?) We're referring in this case to the act of retrieving the data that you've toiled to collect and bringing it into R so we can process it, quality control it, visualize it, analyze it, report on it, and share and archive it. 

---

# Data sources

.pull-left[
- Flat file database
  - e.g., `csv`, `xls`, `tsv`, `txt`, etc.

- Relational database
  - e.g., Access, SQL, etc.

- web API (open or internal)
    - AGOL, ServCat, IRMA, ...
]

.pull-right[
<br>
![:gen 100%, shadow](images/so-many-file-types.jpg)]

???

We distinguish generally between three sources of the data that you might reasonably want to bring into R: flat file databases (things like csvs, Excel and Google spreadsheets, fixed width), relational databases (e.g., Microsoft Access, SQL Server, etc), or data that sitting behind a web API (either open or secured). For flat files and relational databases, R doesn't generally care if that file is sitting on your machine or on a server somewhere as long as ou can construct a path to it.

We simply don't have time to tackle these in depth, so we'll focus on a handy R package for getting data in flat files into R as well as demonstrate that one can pull directly from ArcGIS online into R.

---

# Acquiring data in flat files

.pull-left-70[
- [`rio`](https://cran.r-project.org/web/packages/rio/index.html) 

- [Opinionated defaults](https://github.com/leeper/rio#package-philosophy) simplify data I/O

- `import`, `export`, and `convert` functionality
]

.pull-right-30[.center[![:scale 60%](images/rio-logo.png)]]

---

# [Supported `rio` file formats](https://github.com/leeper/rio#supported-file-formats)

```{r rio-formats, echo = FALSE}
library(DT)
rio_file_types <- rio::import("./data/rio_formats.csv")
DT::datatable(rio_file_types, options = list(pageLength = 6, dom = 'tip'), rownames = FALSE)
```

???

Really a Swiss army knife of data I/O
---

#`rio` example - local file or web URL

```{r flat-file, eval = FALSE}
species <- rio::import("data/species.csv")
species <- rio::import("https://motus.org/data/downloads/api-proxy/species")
```

```{r flat-output, echo = FALSE}
library(knitr); library(kableExtra)
rio::import("https://motus.org/data/downloads/api-proxy/species") %>% 
  select(english, scientific) %>%
  filter(grepl("Red Knot|Painted Bunting|Ruddy Turnston|Saltmarsh", english, TRUE)) %>%
  kable()
```

---

# Acquiring data in relational databases

.pull-left-70[
- [several database architectures supported](https://db.rstudio.com/databases)

- SQL-related options supported generally

  - can execute queries prior to import (`dplyr`)

- Communication with MS Access is problematic
  
  - export flat files (e.g., `csv`, `xlsx`) for `rio`
]

.pull-right-30[
<br>
<br>
.center[![:gen 100%, shadow](images/access.jpg)]]

???

We won't dwell long on relational databases, but there is flexibility. In general, SQL-based options are well-supported. And with the `dplyr` r package that we'll see again later, there's functionality to perform SQL actions in place prior to pulling the data into memory. Getting data directly from Access is an issue, so the best option there is generally to export flat files from Access and work with those...

---

# Acquiring data behind web APIs

- Dedicated ![:scale 3%](images/Rlogo.svg) packages

   - [ScienceBase](https://www.usgs.gov/centers/cdi/science/sbtools-r-package-sciencebase?qt-science_center_objects=0#qt-science_center_objects), [eBird](https://cran.r-project.org/web/packages/auk/index.html), [iNaturalist](https://github.com/ropensci/rinat), [FishBase](https://www.fishbase.in/search.php), [GBIF](https://cran.r-project.org/web/packages/rgbif/index.html), [BISON](https://cran.r-project.org/web/packages/rbison/index.html), [ITIS](https://cran.rstudio.com/web/packages/ritis/)

.center[![:genv 150px, shadow](images/sb.png)  ![:genv 150px, shadow](images/ebird.png)  ![:genv 150px, shadow](images/inaturalist.png)  ![:genv 150px, shadow](images/fishbase.png)]
.center[![:genv 150px, shadow](images/gbif.jpg)  ![:genv 150px, shadow](images/itis.jpg)]

???

Another useful way for us to access data is though a web API service. Put simply, an API is any interface in which we can use a set of programming functions to access features or data from some application. In a web API, that service sits on the web, and we access via HTTP protocols. 

We expect that data access, storage, and archiving via web APIs will become commonplace. For example, we are starting to see more and more of our biological inventory and monitoring work hosted on services like ArcGIS online

For the purposes of this webinar, there is simply too much to cover, so we'll focus on a couple of points. 

FIrst, some web APIs we're likely to find useful have existing dedicated R packages. Services like USGS's ScienceBase (which McCrea will illustrate later), eBird, iNaturalist, FishBase, online biodiversity repositories like GBIF and BISON, as well as ITIS, our taxonomic information system. 

---
count: false

# Acquiring data behind web APIs

- Some APIs already have dedicated ![:scale 3%](images/Rlogo.svg) packages

   - [ScienceBase](https://www.usgs.gov/centers/cdi/science/sbtools-r-package-sciencebase?qt-science_center_objects=0#qt-science_center_objects), [eBird](https://cran.r-project.org/web/packages/auk/index.html), [GBIF](https://cran.r-project.org/web/packages/rgbif/index.html), [BISON](https://cran.r-project.org/web/packages/rbison/index.html), [iNaturalist](https://github.com/ropensci/rinat), [ITIS](https://cran.rstudio.com/web/packages/ritis/)

.pull-left-70[
- Do-It-Yourself

  - AGOL, ServCat, IRMA, PRIMR, data.gov
  
  - [`httr`](https://cran.r-project.org/web/packages/httr/index.html) and [`crul`](https://cran.r-project.org/web/packages/crul/index.html) packages
]

.pull-right-30[
.left[![:gen 90%, shadow](images/servcat-api.jpg)]]
???

But there are plenty of other APIs we're likely to find useful that don't have existing R packages, and we may be interested in setting up access on our own. Services like AGOL...

THere are a couple of handy packages to help you with passing the necessary HTTP requests, like the `httr` and `crul` packages. Although I'll note the  `crul` package is aimed more at developers than casual users.

One complication is that while most web APIs use the same HTTP requests (so most can be accessed using the `httr` package, for example), the format of those requests varies from API to API, and some are more complicated than others. But the rewards for getting it to work can be substantial to a complete code-based workflows. Let's take ArcGIS Online as an example, which the FWS is using increasingly for digital data collection, storage, and archiving of wildlife survey and monitoring data.

---
background-image: url(images/rcw-agol.png)
background-size: contain

<br>

--

.pull-left[.right[![:gen 75%, shadow](images/rcw-surveys.png)]]

--

.pull-right[.left[![:gen 65%, shadow](images/rcw-cluster-survey-app.png)]]

---
count: false
background-image: url(images/rcw-agol.png)
background-size: contain


.center[![:gen 70%, shadow](images/rcw-cluster-survey-stakeholder.png)]

???

Behind a secure web service API, but we can access it...

Compare to going to web browser, navigating to project and specific survey, exporting to a flat file locally, and then reading into R.

---
```{r pull-rcw, include = FALSE, echo = FALSE, eval = FALSE}
pull_rcw_cluster_survey <- function() {

  # Do some stuff...
  
  # Connect and pull content of RCW cluster status feature on USFWS AGOL
  url <- "https://services.arcgis.com/...path_RCW_cluster_feature_server"
  token <- "wouldnt_you_like_to_know"
  outFields <- "*"
  where = "1=1"
  layerInfo <- jsonlite::fromJSON(
    httr::content(httr::POST(url, 
                             query = list(f = "json", token = token), 
                             encode = "form",
                             config = httr::config(ssl_verifypeer = FALSE)), 
                  as = "text"))
  
  # Do some more stuff...
  
}
```

.smaller[
```{r, echo = FALSE}
decorate("pull-rcw") %>%
  flair_rx("httr::content|httr::POST|httr::config", background = "#CBB5FF")
```
]

---
```{r, echo = FALSE}
# source("~/FWS_Projects/Requests_for_assistance/data-mgt-with-r/code/functions/pull_rcw_cluster_survey.R")
# rcw_clusters <- pull_rcw_cluster_survey()
# saveRDS(rcw_cluster, "data/rcw_clusters.rds")
rcw_clusters <- readRDS("data/rcw_clusters.rds")
```

```{r rcw-agol, eval = FALSE}
rcw_clusters <- pull_rcw_cluster_survey()
```

.scroll-output[
```{r rcw-data, echo = FALSE}
rcw_clusters %>%
  select(-globalid, -FK_cluster_ID) %>%
  kable()
```
]

---
count: false

.center[
```{r rcw-plot, echo = FALSE, fig.width=15, fig.height=8}
library(mapview); library(leaflet)
mapview(rcw_clusters, map.types = "OpenStreetMap")@map %>%
  fitBounds(-93, 30, -78, 36)
```
]

---
class: inverse, center, middle, hide-logo

# Process
![](images/life_cycles-data_process.png)


???

---
# Data processing

- Reshaping
- Manipulating, transforming, cleaning
- Quality control
- Exploratory data analysis

.center[![:gen 60%](images/spinning-gold.gif)]

???

One way or the other, we've wrangled our data into R. Now it's time to process that raw data into something fit for visualization and analysis. Way more time is spent on these general tasks of data reshaping, manipulation, cleaning, and quality control than you'd probably be comfortable with. But it's a fact of life, and I like to think this is where we start the process of spinning straw into golden thread...

---
# Data reshaping

.center[[![:gen 60%](images/wide-long.png)](https://www.garrickadenbuie.com/project/tidyexplain/)]

???
In the vast majority of cases, we're working on tabular data. It may be spatially referenced, but there's still likely underlying tabular data association with it. We need to get comfortable moving that tabular data between two distinct presentations---wide and long.

In 'wide' format, each sample/individual occupies a row of the table, and each variable that we've measured on those samples occupies a column. This can be a useful format for recording data, and it's typically the format we need for analyzing data.

In 'long' format, that 'wide' data is pivoted such that all those variables of interest end up in a single column that provides the context for another column that contains the values of that variable. So each sample or individual is represented by multiple rows. While it may see strange, this can be a useful format for summarizing and visualizing data.

So an important data processing task is to reshape data from one form to the other.

---
# Moving from wide to long

.pull-left[
- [`tidyr::pivot_*`](https://tidyr.tidyverse.org/articles/pivot.html) functions (example)
- [`data.table`](https://cran.r-project.org/web/packages/data.table/vignettes/datatable-reshape.html)
]

.pull-right[
.center[![:gen 70%, shadow](images/tidyr-pivots.gif)]
]

???

The most convenient way to perform this action in R is with the `pivot_longer` and `pivot_wider` functions from the tidyr package, though the `data.table` package possess similar functionality.

---
# "Tidy" data

- Not "clean", but organized such that:
  - each variable has its own column
  - each observation has its own row
  - each value has its own cell
  
.center[[![:gen 80%, shadow](images/tidy.png)](https://r4ds.had.co.nz/tidy-data.html)]

???

Somewhat related to wide data is the concept of tidy data.

These three criteria capture all the information available. There's no information encoded in color, or bold font, etc...

Tidy data is our goal as we head towards analysis, though we'll almost certainly have to wrangle the data to get there. FOrtunately, there are R tools for that...

---
# Data manipulation and wrangling

.pull-left[
- Transform variables
- Create new variables
- Filter 
- Sort
- Group and summarize
- Wrangle (clean) factors, strings, dates, times
]

.pull-right[.center[![:gen 80%, shadow](images/tidy-yoda.jpg)]]

---
# Data manipulation and wrangling R packages

.pull-left[
### [Tidyverse](https://www.tidyverse.org/)
- [`tidyr`](https://cran.r-project.org/web/packages/tidyr/index.html) (pivoting, split/unite)
- [`dplyr`](https://cran.r-project.org/web/packages/dplyr/index.html) (data manipulation)
- [`lubridate`](https://cran.r-project.org/web/packages/lubridate/index.html) (dates, times)
- [`forcats`](https://cran.r-project.org/web/packages/forcats/index.html) (factors)
- [`stringr`](https://cran.r-project.org/web/packages/stringr/index.html) (strings)
- facilitates ["piped" workflow](https://r4ds.had.co.nz/pipes.html#pipes)
]

.pull-right[
### Alternative API
- [`data.table`](https://cran.r-project.org/web/packages/data.table/index.html) (acquisition, pivoting, manipulation)
]

---
# Piped workflow

### Traditional (no pipes)
```{r, eval = FALSE}
raw_data <- rio::import("survey_data.xlsx")
mod_data <- dplyr::mutate(raw_data, new_var = var_one + var_two)
mod_data <- dplyr::filter(mod_data, new_var < 100)
mod_data <- dplyr::arrange(mod_data, sort_var)
```

---
count: false

# Piped workflow

- Pipe operator: `%>%` 

### Piped

```{r piped, include = FALSE, eval = FALSE}
raw_data <- rio::import("survey_data.xlsx")
mod_data <- raw_data %>%
  dplyr::mutate(new_var = var_one + var_two) %>%
  dplyr::filter(new_var < 100) %>%
  dplyr::arrange(sort_var)
```

.smaller[
```{r, echo = FALSE}
decorate("piped", eval = FALSE) %>%
  flair("%>%", background = "#CBB5FF")
```
]
---
# Quality control

.pull-left[
- Passive: <ins>**data profiling**</ins>
- Active: <ins>**assertive programming**</ins>
]

.pull-right[.center[![:gen 70%, shadow](images/quality-control-is.jpg)]]

<br>
<br>
<br>
<br>
<br>
<br>

Small mammal trapping data set from [Portal Project Teaching Database](https://figshare.com/articles/Portal_Project_Teaching_Database/1314459)

```{r, eval = FALSE}
mammals <- rio::import("https://ndownloader.figshare.com/files/10717177")
```

???

We think about quality control in terms of a more passive, ad hoc approach (which we refer to as data profiling), and a more explicit, active quality control we refer to as assertive programming. 

Data profiling is passive in that it doesn't involve testing any specific aspects of the data, but rather uses informative summaries to help you identify or flag potential issues like questionable data entries, extreme/unlikely values, and patterns of missing data

On the other hand, assertive programming turns your knowledge of the data into explicit tests to make sure the data meets your requirements

---
# Data profiling functions

## `str` & `summary` (base ![:scale 3%](images/Rlogo.svg))

```{r, echo=FALSE}
mammals <- rio::import("data/small_mammals.csv")
```

```{r}
str(mammals)
```

???

We'll take the more passive data profiling approach first, illustrating some R functions to help us along. The first two examples are part of base R (not in any special package but part of the core R software)

---
.scroll-output-tall[.smallest[
```{r}
summary(mammals)
```
]]

---
## `summarytools::dfSummary`

```{r, eval = FALSE}
summarytools::view(summarytools::dfSummary(mammals))
```

.scroll-output[.smaller[
```{r, echo = FALSE}
print(summarytools::dfSummary(mammals, valid.col = FALSE, graph.magnif = 0.75), method = "render")
```
]]

???

Three additional functions we've found useful give us a fairly robust profile of our data without much work. I'll speed through them a bit since they contain similar types of information with some variation in how the summaries are presented. The first is the `dfSummary` function from the `summarytools` package.

---
## `skimr::skim`

.scroll-output[.smallest[
```{r, fig.width = 15}
skimr::skim(mammals)
```
]]

---
## `DataExplorer::create_report`

```{r, eval = FALSE}
# An .html file is output to your current working directory and opens in browser
DataExplorer::create_report(mammals, 
                            output_file = "mammal_data_profile.html")
```

---
.scroll-output-tall[.smallest[
```{r, echo = FALSE}
htmltools::includeHTML("rmd/mammal_report.html")
```
]]  

???

More graphical presentation, plus some additional looks at covariation among your data. Each of these last three functions have additional functionality (for example, exploring data profiles by groups), so I encourage to dig a little deeper with these functions that give you a fair bit of useful information for very little overhead...

---
# Assertive programming

- [`assertr`](https://cran.r-project.org/web/packages/assertr/index.html) ([vignette](https://cran.r-project.org/web/packages/assertr/vignettes/assertr.html))

.pull-left-70[
```{r, eval = FALSE}
# Expect maximum weight
mammals %>% 
  assertr::verify(weight < 100)

# Alternative, giving weight range
mammals %>% 
  assertr::assert(assertr::within_bounds(15, 100), weight)

# Confirm expected format of sex variable
mammals %>% 
  assertr::assert(assertr::in_set(c("M", "F")))
```
]

.pull-right-30[
![:gen 100%, shadow](images/assertive.jpg)]

???
SO that's data profiling. A good approach to take a general pulse check of your data. But in many, if not most cases, we should be very familiar with the data we've collected and are now trying to wrangle. So we should have some pretty strong intuitions or ideas of what the data *should* look like. In those cases, we can go on the quality control offensive, and be more explicit and assertive to detect problematic records as soon as possible in the processing workflow. 

The `assertr` package gives us a suite of functions designed to help us test and verify our intuitions and assumptions about the data. I only illustrate three very simple examples, such as checking that are small mammal weights meet a couple of criteria, or that we recorded sex in a consistent format. But there are many, many more possibilities, so we encourage you to check out the linked vignette

---
# Exploratory data analysis

.pull-left-70[
- Aims for understanding
  - variation, missing values, covariation
- Iterative (questions -> answers -> questions)
- Visualization is key
]

.pull-right-30[
![:gen 90%, shadow](images/visualization.jpg)]

???
Once we've wrangled the data to our liking, and finished our QC due diligence, we can be more thoughtful and intentional with our data, and attempt to start understanding it with exploratory data analysis. EDA is often an iterative process of asking questions and seeking answers from your data, and it often involves a lot of visualization. So that's were we focus this abbreviated coverage---with a quick look at the most popular visualization framework in R---ggplot2

---
# Visualization framework

- Flexible, consistent grammar for visualization
  - [`ggplot2`](https://cran.r-project.org/web/packages/ggplot2/index.html): a grammar of graphics	
  
.center[![:genv 375px, shadow](images/ggplot-layers.png)]

???
Graphics are built by layering independent elements, such as:
  - data
  - aesthetic mapping (position, color, fill, shape, size, linetype, etc.)
  - geometric object (points, lines, bars, rasters, etc.)
  - scales
  - faceting (separate graphics by groups)
  - coordinate systems
  - labels
  - themes
...among others 

There are alternative visualization tools in R, but we appreciate the flexibility and intuitiveness of ggplot, and how quickly you can go from data to publication quality figures. Couple that with the relative ease of turning ggplot-based figures into animations and interactive graphics, and you've got a useful tool. I'll hope a quick demonstration of plot building illustrates the idea of building a plot in layers, point you to a good resource to learn more, and show you a few figures created by ggplot to illustrate the range of possibilities. 

---


### Target output: *Submerged aquatic vegetation - Mattamuskeet NWR*

.center[![:genv 500px, shadow](images/sav.png)]

```{r, echo = FALSE, eval = TRUE, message = FALSE, warning = FALSE}
library(sf); library(ggplot2)
sav_pts <- readRDS("./data/sav_obs.rds")
sav_model_fit <- readRDS("./data/gridded_sav.rds")
LM <- readRDS("./data/LM.rds")
theme_set(theme_gray(base_size = 20))
```

---
.pull-left-60[.smallest[
```{r, echo = TRUE, eval = FALSE}
ggplot(data = sav_model_fit) #<<
```
]]

.pull-right-40[
```{r, echo = FALSE, eval = TRUE, fig.width=8, fig.height=4}
ggplot(data = sav_model_fit)
```
]

---
count: false

.pull-left-60[.smallest[
```{r, echo = TRUE, eval = FALSE}
ggplot(data = sav_model_fit) + 
  geom_raster(aes(x = x, y = y, fill = sav)) #<<
```
]]

.pull-right-40[
```{r, echo = FALSE, eval = TRUE, fig.width=8, fig.height=4}
ggplot(data = sav_model_fit) + 
  geom_raster(aes(x = x, y = y, fill = sav))
```
]

---
count: false

.pull-left-60[.smallest[
```{r, echo = TRUE, eval = FALSE}
ggplot(data = sav_model_fit) + 
  geom_raster(aes(x = x, y = y, fill = sav)) + 
  geom_sf(data = LM, color = "black", fill = NA) #<<
```
]]

.pull-right-40[
```{r, echo = FALSE, eval = TRUE, fig.width=8, fig.height=4}
ggplot(data = sav_model_fit) + 
  geom_raster(aes(x = x, y = y, fill = sav)) + 
  geom_sf(data = LM, color = "black", fill = NA)
```
]

---
count: false

.pull-left-60[.smallest[
```{r, echo = TRUE, eval = FALSE}
ggplot(data = sav_model_fit) + 
  geom_raster(aes(x = x, y = y, fill = sav)) + 
  geom_sf(data = LM, color = "black", fill = NA) +
  geom_point(data = sav_pts, shape = 21, #<< 
             aes(x, y, fill = sav)) #<<
```
]]

.pull-right-40[
```{r, echo = FALSE, eval = TRUE, fig.width=8, fig.height=4}
ggplot(data = sav_model_fit) + 
  geom_raster(aes(x = x, y = y, fill = sav)) + 
  geom_sf(data = LM, color = "black", fill = NA) +
  geom_point(data = sav_pts, shape = 21, 
             aes(x, y, fill = sav))
```
]

---
count: false

.pull-left-60[.smallest[
```{r, echo = TRUE, eval = FALSE}
ggplot(data = sav_model_fit) + 
  geom_raster(aes(x = x, y = y, fill = sav)) + 
  geom_sf(data = LM, color = "black", fill = NA) +
  geom_point(data = sav_pts, shape = 21, 
             aes(x, y, fill = sav)) + 
  scale_fill_viridis_c("Proportion\nSAV", #<< 
                     breaks = c(0,0.1,0.4,0.7,1), #<<
                     limits = c(0,1)) #<<
```
]]

.pull-right-40[
```{r, echo = FALSE, eval = TRUE, fig.width=8, fig.height=4}
ggplot(data = sav_model_fit) + 
  geom_raster(aes(x = x, y = y, fill = sav)) + 
  geom_sf(data = LM, color = "black", fill = NA) +
  geom_point(data = sav_pts, shape = 21, 
             aes(x, y, fill = sav)) + 
  scale_fill_viridis_c("Proportion\nSAV", 
                     breaks = c(0,0.1,0.4,0.7,1), 
                     limits = c(0,1))
```
]

---
count: false

.pull-left-60[.smallest[
```{r, echo = TRUE, eval = FALSE}
ggplot(data = sav_model_fit) + 
  geom_raster(aes(x = x, y = y, fill = sav)) + 
  geom_sf(data = LM, color = "black", fill = NA) +
  geom_point(data = sav_pts, shape = 21, 
             aes(x, y, fill = sav)) + 
  scale_fill_viridis_c("Proportion\nSAV", 
                     breaks = c(0,0.1,0.4,0.7,1), 
                     limits = c(0,1)) +
  coord_sf() #<<
```
]]

.pull-right-40[
```{r, echo = FALSE, eval = TRUE, fig.width=8, fig.height=4}
ggplot(data = sav_model_fit) + 
  geom_raster(aes(x = x, y = y, fill = sav)) + 
  geom_sf(data = LM, color = "black", fill = NA) +
  geom_point(data = sav_pts, shape = 21, 
             aes(x, y, fill = sav)) + 
  scale_fill_viridis_c("Proportion\nSAV", 
                     breaks = c(0,0.1,0.4,0.7,1), 
                     limits = c(0,1)) +
  coord_sf()
```
]

---
count: false

.pull-left-60[.smallest[
```{r, echo = TRUE, eval = FALSE}
ggplot(data = sav_model_fit) + 
  geom_raster(aes(x = x, y = y, fill = sav)) + 
  geom_sf(data = LM, color = "black", fill = NA) +
  geom_point(data = sav_pts, shape = 21, 
             aes(x, y, fill = sav)) + 
  scale_fill_viridis_c("Proportion\nSAV", 
                     breaks = c(0,0.1,0.4,0.7,1), 
                     limits = c(0,1)) +
  coord_sf() +
  labs(title = "SAV in Lake Mattamuskeet", #<<
       subtitle = "2004", #<<
       x = NULL, y = NULL) #<<
```
]]

.pull-right-40[
```{r, echo = FALSE, eval = TRUE, fig.width=8, fig.height=4}
ggplot(data = sav_model_fit) + 
  geom_raster(aes(x = x, y = y, fill = sav)) + 
  geom_sf(data = LM, color = "black", fill = NA) +
  geom_point(data = sav_pts, shape = 21, 
             aes(x, y, fill = sav)) + 
  scale_fill_viridis_c("Proportion\nSAV", 
                     breaks = c(0,0.1,0.4,0.7,1), 
                     limits = c(0,1)) +
  coord_sf() +
  labs(title = "SAV in Lake Mattamuskeet",
       subtitle = "2004",
       x = NULL, y = NULL)
```
]

---
count: false

.pull-left-60[.smallest[
```{r, echo = TRUE, eval = FALSE}
ggplot(data = sav_model_fit) + 
  geom_raster(aes(x = x, y = y, fill = sav)) + 
  geom_sf(data = LM, color = "black", fill = NA) +
  geom_point(data = sav_pts, shape = 21, 
             aes(x, y, fill = sav)) + 
  scale_fill_viridis_c("Proportion\nSAV", 
                     breaks = c(0,0.1,0.4,0.7,1), 
                     limits = c(0,1)) +
  coord_sf() +
  labs(title = "SAV in Lake Mattamuskeet",
       subtitle = "2004",
       x = NULL, y = NULL) +
  theme_bw() #<<
```
]]

.pull-right-40[
```{r, echo = FALSE, eval = TRUE, fig.width=8, fig.height=4}
ggplot(data = sav_model_fit) + 
  geom_raster(aes(x = x, y = y, fill = sav)) + 
  geom_sf(data = LM, color = "black", fill = NA) +
  geom_point(data = sav_pts, shape = 21, 
             aes(x, y, fill = sav)) + 
  scale_fill_viridis_c("Proportion\nSAV", 
                     breaks = c(0,0.1,0.4,0.7,1), 
                     limits = c(0,1)) +
  coord_sf() +
  labs(title = "SAV in Lake Mattamuskeet",
       subtitle = "2004",
       x = NULL, y = NULL) +
  theme_bw(base_size = 20)
```
]

```{css, echo = FALSE}
.box{
  text-align: center;
  float:left;
  margin-right:20px;
}
.clear{
    clear:both;
}
```

--

.pull-left[.right[<iframe width="356" height="200" src="https://www.youtube.com/embed/h29g21z0a68" frameborder="0" allowfullscreen></iframe>]]
.pull-right[.left[<iframe width="356" height="200" src="https://www.youtube.com/embed/0m4yywqNPVY" frameborder="0" allowfullscreen></iframe>]]

---

.center[![:gen 80%, shadow](images/ggeg-dabblers.png)]

---
count:false

.center[![:gen 50%, shadow](images/ggeg-nansound.png)]

---
count: false

.center[![:gen 70%, shadow](images/ggeg-waterquality.png)]

---
count: false

.center[![:gen 75%, shadow](images/ggeg-duck_models.png)]

---
count: false

.center[![:gen 60%, shadow](images/ggeg-REKN.png)]

---
class: inverse, center, middle, hide-logo

# Analyze

???

We've made to analysis. We now have some idea of the modelling approach we want to take. ANd that's probably different for every person wathcih. So instead of picking favorites, we'll focus on a few good resources to lead you down the right code-based path in R for whatever analysis you're looking for. The good news is that, if it can be done, there's likely R resources available.

---
# CRAN task views

- relevant R packages for ~ 40 topics

- emphasize packages, not methodologies 

- not endorsements

---
# Animal tracking example

```{r, echo = FALSE, fig.width=15}
include_url("https://cran.r-project.org/web/views/Tracking.html")
```

---
# Diverse topics

.scroll-output[
<table summary="CRAN Task Views">
  <tbody><tr valign="top">
    <td><a href="https://cran.r-project.org/web/views/Bayesian.html">Bayesian</a></td>
    <td>Bayesian Inference</td>
  </tr>
  <tr valign="top">
    <td><a href="https://cran.r-project.org/web/views/ChemPhys.html">ChemPhys</a></td>
    <td>Chemometrics and Computational Physics</td>
  </tr>
  <tr valign="top">
    <td><a href="https://cran.r-project.org/web/views/ClinicalTrials.html">ClinicalTrials</a></td>
    <td>Clinical Trial Design, Monitoring, and Analysis</td>
  </tr>
  <tr valign="top">
    <td><a href="https://cran.r-project.org/web/views/Cluster.html">Cluster</a></td>
    <td>Cluster Analysis &amp; Finite Mixture Models</td>
  </tr>
  <tr valign="top">
    <td><a href="https://cran.r-project.org/web/views/Databases.html">Databases</a></td>
    <td>Databases with R</td>
  </tr>
  <tr valign="top">
    <td><a href="https://cran.r-project.org/web/views/DifferentialEquations.html">DifferentialEquations</a></td>
    <td>Differential Equations</td>
  </tr>
  <tr valign="top">
    <td><a href="https://cran.r-project.org/web/views/Distributions.html">Distributions</a></td>
    <td>Probability Distributions</td>
  </tr>
  <tr valign="top">
    <td><a href="https://cran.r-project.org/web/views/Econometrics.html">Econometrics</a></td>
    <td>Econometrics</td>
  </tr>
  <tr valign="top">
    <td><a href="https://cran.r-project.org/web/views/Environmetrics.html">Environmetrics</a></td>
    <td>Analysis of Ecological and Environmental Data</td>
  </tr>
  <tr valign="top">
    <td><a href="https://cran.r-project.org/web/views/ExperimentalDesign.html">ExperimentalDesign</a></td>
    <td>Design of Experiments (DoE) &amp; Analysis of Experimental Data</td>
  </tr>
  <tr valign="top">
    <td><a href="https://cran.r-project.org/web/views/ExtremeValue.html">ExtremeValue</a></td>
    <td>Extreme Value Analysis</td>
  </tr>
  <tr valign="top">
    <td><a href="https://cran.r-project.org/web/views/Finance.html">Finance</a></td>
    <td>Empirical Finance</td>
  </tr>
  <tr valign="top">
    <td><a href="https://cran.r-project.org/web/views/FunctionalData.html">FunctionalData</a></td>
    <td>Functional Data Analysis</td>
  </tr>
  <tr valign="top">
    <td><a href="https://cran.r-project.org/web/views/Genetics.html">Genetics</a></td>
    <td>Statistical Genetics</td>
  </tr>
  <tr valign="top">
    <td><a href="https://cran.r-project.org/web/views/Graphics.html">Graphics</a></td>
    <td>Graphic Displays &amp; Dynamic Graphics &amp; Graphic Devices &amp; Visualization</td>
  </tr>
  <tr valign="top">
    <td><a href="https://cran.r-project.org/web/views/HighPerformanceComputing.html">HighPerformanceComputing</a></td>
    <td>High-Performance and Parallel Computing with R</td>
  </tr>
  <tr valign="top">
    <td><a href="https://cran.r-project.org/web/views/Hydrology.html">Hydrology</a></td>
    <td>Hydrological Data and Modeling</td>
  </tr>
  <tr valign="top">
    <td><a href="https://cran.r-project.org/web/views/MachineLearning.html">MachineLearning</a></td>
    <td>Machine Learning &amp; Statistical Learning</td>
  </tr>
  <tr valign="top">
    <td><a href="https://cran.r-project.org/web/views/MedicalImaging.html">MedicalImaging</a></td>
    <td>Medical Image Analysis</td>
  </tr>
  <tr valign="top">
    <td><a href="https://cran.r-project.org/web/views/MetaAnalysis.html">MetaAnalysis</a></td>
    <td>Meta-Analysis</td>
  </tr>
  <tr valign="top">
    <td><a href="https://cran.r-project.org/web/views/MissingData.html">MissingData</a></td>
    <td>Missing Data</td>
  </tr>
  <tr valign="top">
    <td><a href="https://cran.r-project.org/web/views/ModelDeployment.html">ModelDeployment</a></td>
    <td>Model Deployment with R</td>
  </tr>
  <tr valign="top">
    <td><a href="https://cran.r-project.org/web/views/Multivariate.html">Multivariate</a></td>
    <td>Multivariate Statistics</td>
  </tr>
  <tr valign="top">
    <td><a href="https://cran.r-project.org/web/views/NaturalLanguageProcessing.html">NaturalLanguageProcessing</a></td>
    <td>Natural Language Processing</td>
  </tr>
  <tr valign="top">
    <td><a href="https://cran.r-project.org/web/views/NumericalMathematics.html">NumericalMathematics</a></td>
    <td>Numerical Mathematics</td>
  </tr>
  <tr valign="top">
    <td><a href="https://cran.r-project.org/web/views/OfficialStatistics.html">OfficialStatistics</a></td>
    <td>Official Statistics &amp; Survey Methodology</td>
  </tr>
  <tr valign="top">
    <td><a href="https://cran.r-project.org/web/views/Optimization.html">Optimization</a></td>
    <td>Optimization and Mathematical Programming</td>
  </tr>
  <tr valign="top">
    <td><a href="https://cran.r-project.org/web/views/Pharmacokinetics.html">Pharmacokinetics</a></td>
    <td>Analysis of Pharmacokinetic Data</td>
  </tr>
  <tr valign="top">
    <td><a href="https://cran.r-project.org/web/views/Phylogenetics.html">Phylogenetics</a></td>
    <td>Phylogenetics, Especially Comparative Methods</td>
  </tr>
  <tr valign="top">
    <td><a href="https://cran.r-project.org/web/views/Psychometrics.html">Psychometrics</a></td>
    <td>Psychometric Models and Methods</td>
  </tr>
  <tr valign="top">
    <td><a href="https://cran.r-project.org/web/views/ReproducibleResearch.html">ReproducibleResearch</a></td>
    <td>Reproducible Research</td>
  </tr>
  <tr valign="top">
    <td><a href="https://cran.r-project.org/web/views/Robust.html">Robust</a></td>
    <td>Robust Statistical Methods</td>
  </tr>
  <tr valign="top">
    <td><a href="https://cran.r-project.org/web/views/SocialSciences.html">SocialSciences</a></td>
    <td>Statistics for the Social Sciences</td>
  </tr>
  <tr valign="top">
    <td><a href="https://cran.r-project.org/web/views/Spatial.html">Spatial</a></td>
    <td>Analysis of Spatial Data</td>
  </tr>
  <tr valign="top">
    <td><a href="https://cran.r-project.org/web/views/SpatioTemporal.html">SpatioTemporal</a></td>
    <td>Handling and Analyzing Spatio-Temporal Data</td>
  </tr>
  <tr valign="top">
    <td><a href="https://cran.r-project.org/web/views/Survival.html">Survival</a></td>
    <td>Survival Analysis</td>
  </tr>
  <tr valign="top">
    <td><a href="https://cran.r-project.org/web/views/TeachingStatistics.html">TeachingStatistics</a></td>
    <td>Teaching Statistics</td>
  </tr>
  <tr valign="top">
    <td><a href="https://cran.r-project.org/web/views/TimeSeries.html">TimeSeries</a></td>
    <td>Time Series Analysis</td>
  </tr>
  <tr valign="top">
    <td><a href="https://cran.r-project.org/web/views/Tracking.html">Tracking</a></td>
    <td>Processing and Analysis of Tracking Data</td>
  </tr>
  <tr valign="top">
    <td><a href="https://cran.r-project.org/web/views/WebTechnologies.html">WebTechnologies</a></td>
    <td>Web Technologies and Services</td>
  </tr>
  <tr valign="top">
    <td><a href="https://cran.r-project.org/web/views/gR.html">gR</a></td>
    <td>gRaphical Models in R</td>
  </tr>
</tbody></table>
]

---
# When all else fails...

.center[![:gen 60%, shadow](images/google-it.jpg)]


---
class: center, middle, inverse, hide-logo
# Sharing and Archiving

![](images/life_cycles-data_share-archiving.png)


???
Last but not least, I was cover the sharing and archiving steps of the data life cycle.

---
class: center, middle, hide-logo
background-image: url("images/report_about_reporting.jpg")
background-size: contain


???
Or in other words, I will attempt to report about reporting...


---
background-image: url("images/rmarkdown_logo.png")
background-position: 85% 50%
background-size: 40%
# Literate programming

.pull-left-70[
.large[You can generate reports directly  
from RStudio!]

**Allows for:**
- Code to be directly inserted into document
- Multiple outputs
- Easy to use
- Increased reproducibility
- Fewer sources of errors
]


???
As I mentioned earlier, scientific writing and coding were traditionally separate activities. For example, a scientist who wants to use code to generate a figure for her report would have the code for generating that figure in one file and the document itself in another. 

This is a challenge for reproducibility and keeping track of where each piece of data came from and whether it's up-to-date. “Literate programming” provides an alternative approach, whereby code and text are interleaved within a single file, which can be processed by software to produce documents with the output of the code (e.g. figures, tables, and summary statistics) automatically interwoven with the document’s body text. A common literate programming file format in R is call RMarkdown

Unlike cumbersome word processing applications, text written in Markdown can be easily shared between computers, mobile phones, and people. It’s quickly becoming the writing standard for academics and scientists. This approach has several advantages. For one, the code output of a literate programming document is by definition guaranteed to be consistent with the code in the document’s source. At the same time, literate programming can make it easier to develop analyses by reducing the separation between writing and coding.


---
class: middle, center
background-image: url("images/rmarkdown_wizards.png")
background-size: contain


???
How does it work? Well, unless you really want to get into the weeds...given our time constrainst today, let's just say that it's like magic! As shown here, you start by adding text and what's called "chunks" of code to an Rmarkdown document. You then what's called "knit" the file by running the `knit` function or clicking the `knit` button in RMarkdown. Magic happens and you can celebrate the percieved wizardy. A document is produced in the file format of your choosing, generally html, Word, or PDF.


---
# ![:scale 5%](images/rmarkdown_logo.png) The YAML header

````{r, eval=FALSE}
---
title: "My report"
author: "McCrea Cobb"
date: "6/24/2020"
output: pdf_document
---
````


???
So what does an RMarkdown file contain? There are two required sections: a header and the main body. As shown here, a simple header (written in YAML- Yet Another Markup Language) generally contains some metadata and the desired file output. 


---
# ![:scale 5%](images/rmarkdown_logo.png) A more complex header

.smaller[
````{r complex_header, eval=FALSE, include=FALSE}
---
title: |
    | ![](usfws_refuges_logos.png){width=5cm}
    | 
    | \LARGE Region 4 Inventory and Monitoring Branch 
subtitle: |
    | \Large Mobile Acoustical Bat Monitoring
    | \Large Annual Summary Report
author: '`r params$year`'
date: '`r params$station`'
output:
  pdf_document:
    includes:
      in_header: MABM_report_preamble.tex
urlcolor: blue
params:
  year: 0 # placeholder 
  station: placeholder
  stn_start_yr: 0 # placeholder
  route_path: placeholder
  survey_path: placeholder
  bat_path: placeholder
  spp_path: placeholder
  out_dir: placeholder
  goog_API_key: placeholder
---

````

```{r, echo=FALSE}
decorate("complex_header", eval=FALSE) %>%
  flair("params")
```
]

???
Header can get more complex as the document gets more customized. This one includes input parameters (`params`) that allow the user to customize the content of the report based on their input values. In this report, the user select the year of data and refuge station on which to generate a report.


---
.pull-left[
.small[
`####` Some Rmarkdown code

This is an `**`R Markdown`**` document. You can write inline code like this: 1 + 1 = `` `r
1 + 1` ``.

You can embed an R code chunk like this:

    ```{r eval=TRUE, echo=FALSE}`r ''`
    DT::datatable(iris[1:3], 
                  rownames = FALSE,
                  options = list(pageLength = 3))
    ```

`####` Including Figures

You can also embed figures, for example:

    ```{r, eval=TRUE, echo=FALSE}`r ''`
    library(leaflet)

    leaflet() %>%
      addTiles() %>%
      addMarkers(lng = 174.768, 
                 lat = -36.852, 
                 popup = "The birthplace of R")
    ```

]
]


???
The meat of the report happens in the main body of an RMarkdown document. As I mentioned earlier, the main body, as shown here, is a combination of text written in markdown format and embedded R code that are either included as distinct chunks surrounded by three backticks or in-line surrounded by one backtick. 


---
.pull-left[
.small[
`####` Some Rmarkdown code

This is an `**`R Markdown`**` document. You can write inline code like this: 1 + 1 = `` `r
1 + 1` ``.

You can embed an R code chunk like this:

    ```{r eval=TRUE, echo=FALSE}`r ''`
    DT::datatable(iris[1:3], 
                  rownames = FALSE,
                  options = list(pageLength = 3))
    ```

`####` Including Figures

You can also embed figures, for example:

    ```{r, eval=TRUE, echo=FALSE}`r ''`
    library(leaflet)

    leaflet() %>%
      addTiles() %>%
      addMarkers(lng = 174.768, 
                 lat = -36.852, 
                 popup = "The birthplace of R")
    ```

]
]

.pull-right[
.small[
#### Some Rmarkdown code
This is an **R Markdown** document. You can write inline code like this: 1 + 1 = `r 1+1`.

You can embed an R code chunk like this:
]

.smaller[
```{r, echo=FALSE}
DT::datatable(iris[1:3], 
              rownames = FALSE,
              options = list(pageLength = 3))
```
]

.small[
#### Including Figures

You can also embed figures, for example:

```{r pressure, echo=FALSE, fig.width=6, fig.height=2, warning=FALSE}
library(leaflet)

leaflet() %>%
  addTiles() %>%
  addMarkers(lng=174.768, lat=-36.852, popup="The birthplace of R")

```
]
]


???
When the file is knitted, the result is a document that includes the text and the output of the R code. In this example, the R code chunk produced a table and a map. The in-line code calculated a value (1 + 1).


---

```{r, echo=FALSE, out.width="100%", out.height="100%"}
knitr::include_graphics("./images/rmarkdown_report.pdf")
```


???
Here's an example of an RMarkdown PDF report that is automatically generated every week to summarize GPS collar data for quality control. As data regularly upload from the collars to the satellites to servers, the tables and figures in the report are also updated.


---
# ![:scale 5%](images/xaringan_logo.png) Presentations 

.center[![:scale 80%](images/rmarkdown_presentation.png)]

???
As mentioned earlier, Rmarkdown documents can be rendered into multiple different formats.
You can even create presentations, such as the one that you are looking at right now!


---
background-image: url("images/shiny.png")
background-position: 90% 50%
background-size: 40%
# Web applications 

<br>
.pull-left[
.large[
- Build interactive web apps from R

- Host standalone apps online

- Embed them in RMarkdown docs
]
]

???
You can also generate interactive content from R, such as websites and dashboards. The Shiny package allows for these, which can also be imbedded into Rmarkdown outputs.

Shiny is an R package that provides a framework for building interactive applications using R, without HTML, CSS, or JavaScript knowledge.

You can host standalone apps as webpages, or embed them in rmarkdown documents


---
class: center, middle, hide-logo

<video controls autoplay width="1200">
  <source src="images/collar_viewer.mp4" type="video/mp4">
</video>


???
Here's an example of a shiny app in action. In this example, the user pulls the shiny app from GitHub and runs it locally. After providing data, the app maps GPS collar data, in this case from lynx in Alaska. The user can than select a subset of GPS collars, create home ranges, and do thing like measure the diameter of the home range.


---


```{r, echo=FALSE}
knitr::include_app("https://vac-lshtm.shinyapps.io/ncov_tracker/?_ga=2.142693877.1775727125.1591996474-181506872.1532128536)", height="600px")
```



???

---
background-image:url("images/cher_data.jpg")
background-position: 50% 80%
# `r icon::fa("file-archive")` Archiving 


???
Ok, now for the final and arguably the most important step that is often overlooked in the data life cycle: archiving. For the typical scientific data project, this should the raw data and the code required to reproduce the results.

Typically the process to do this is to visit a repository website and then manually upload data and metadata.
Our take home here, is that if you are using R, it is possible for you can include code to save your data and products in a remote secure repository.
- For example, instead of manually opening ServCat and saving your reports to as a product in a ServCat record, it is possible to use R code to save results to ServCat. 
- This is an active area of development.
- I encourage you to reach out to your data managers and ServCat managers to look into this more.


---
background-image:url("images/cher_data.jpg")
background-position: 50% 80%
# `r icon::fa("file-archive")` Archiving 

.center[![:gen 35%, shadow](images/we_can_do_it_in_r.png)]


???
It is possible to do this manual step programmically in R!

Just as Adam discussed about R's ability to *access* data from remote repositories, the opposite is also true; R code can also be used to *save* your data and products to a remote secure repository.

- Although it is possible to use web services to get, put and delete data in a remote repository, this is often not straightforward. - For example, instead of manually opening ServCat and saving your reports to as a product in a ServCat record, it is *possible* to use R code to save results to ServCat. Emphasis on "possible".

To aid in this process, custom R packages have been built to access and archive scientific data repositories, such as ScienceBase. These make it much easier to use the repositories web services.

This is an active area of development. I encourage you to reach out to your data managers and ServCat managers to look into this more option for your repository of interest.

Today I was cover an example of an R package called `sbtools` that was developed for interfacing R with data services from ScienceBase, the USGS data repository.


---
# ![:scale 5%](images/sciencebase_logo.png) [`sbtools`](https://journal.r-project.org/archive/2016-1/winslow-chamberlain-appling-etal.pdf) 

.pull-left[
The tool to allow complete access to the USGS ScienceBase API from R. It supports the creation, editing and access of data/metadata:

.small[
```{r, echo=FALSE}
decorate('
library(sbtools)

# Create new item (record), by default under "My Items" parent
new_item <- item_create(title = "new test item")
', eval=FALSE) %>%
  flair("item_create") %>%
  flair("sbtools", background="pink")
```
]
]

.pull-right[
![](images/sciencebase.PNG)
]


???
`sbtools` enables direct access to the advanced online data functionality provided by ScienceBase, the U.S. Geological Survey’s online scientific data storage platform.
- Provides scripted R access to ScienceBase to manage metadata and data files, to search the catalog of datasets, and to view and modify data in formats familiar to R users.


---
# ![:scale 5%](images/sciencebase_logo.png) [`sbtools`](https://journal.r-project.org/archive/2016-1/winslow-chamberlain-appling-etal.pdf) 

.pull-left[
The tool to allow complete access to the USGS ScienceBase API from R. It supports the creation, editing and access of data/metadata:

.small[
```{r, echo=FALSE}
decorate('
library(sbtools)

# Create new item (record), by default under "My Items" parent
new_item <- item_create(title = "new test item")
', eval=FALSE) %>%
  flair("item_create") %>%
  flair("sbtools", background="pink")
```
]

Once an item is created, an authenticated user can edit the metadata or attach data files to that item:

.small[
```{r, echo=FALSE}
decorate('
# Give the item a new title
edited_item <-  item_update(new_item, list(title = "new updated item"))

# Append data to the item
item_list_files(new_item)
', eval=FALSE) %>%
  flair("item_update") %>%
  flair("item_list_files")
```
]
]

.pull-right[
![](images/sciencebase.PNG)
]


???
`sbtools` enables direct access to the advanced online data functionality provided by ScienceBase, the U.S. Geological Survey’s online scientific data storage platform.
- Provides scripted R access to ScienceBase to manage metadata and data files, to search the catalog of datasets, and to view and modify data in formats familiar to R users.


---
# ![:scale 5%](images/sciencebase_logo.png) [`sbtools`](https://journal.r-project.org/archive/2016-1/winslow-chamberlain-appling-etal.pdf) - Accessing data

```{r, echo=FALSE}
decorate('
# Access the record
test_item <- item_get("572a2a7fe4b0b13d391a0f6c")

# Take a look at the citation
test_item
', comment = "") %>%
  flair("item_get")
```


???


---

<style>
.leaflet {
    margin: auto;
}
</style>

.small[
```{r map_moose, include=FALSE, warning=FALSE, fig.height=5, fig.width= 10, fig.align='center'}
library(leaflet)

# Get study area boundaries
study_area <- item_get_wfs(test_item)

# Map it
leaflet() %>%
  addPolygons(data = study_area@polygons[[1]], 
              label = study_area$PopupInfo) %>%
  addTiles()
```


```{r, echo=FALSE, warning=FALSE}
decorate("map_moose") %>%
  flair("%>%", background="pink") %>%
  flair("item_get_wfs") %>%
  flair("leaflet")
```
]

???


---
class: center, middle, hide-logo
background-image: url("images/demo_thinking.jpg")
background-size: cover


???
Time for the demo.


---
# Case study: Mobile Acoustical Bat Monitoring

- 86 survey routes at 63 FWS field stations (~ 40 participate annually)
- Data life cycle bottlenecks
  - process bat software output for entry into relational database
  - annual reports for each participating station

.center[![:gen 38%, shadow](images/MABM_route_centroids_bw.png)]